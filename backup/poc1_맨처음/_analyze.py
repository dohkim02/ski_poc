from prompt import ANALYZE_SYSTEM_PROMPT
import pandas as pd
import json
from pydantic import BaseModel
from typing import Dict, List
import sys
import os
import re
import asyncio
from tqdm.asyncio import tqdm_asyncio

# model.pyê°€ ìˆëŠ” í´ë” ê²½ë¡œ
MODEL_PATH = os.path.abspath("../")  # ì˜ˆ: í•œ ë‹¨ê³„ ë°”ê¹¥ í´ë”
sys.path.append(MODEL_PATH)
from model import initialize_llm


class EvaluationInfo(BaseModel):
    result: str  # íŒë‹¨ ê²°ê³¼ (ex: "ì •ìƒ", "ì´ìƒ", ì„¤ëª… í¬í•¨ ê°€ëŠ¥)
    reason: str  # íŒë‹¨ ì´ìœ 


class EvaluationOutput(BaseModel):
    user_input: Dict  # ì‚¬ìš©ì ì§ˆë¬¸ ì „ì²´ ì›ë³¸
    usage_evaluation: EvaluationInfo  # ìš©ë„ íŒë‹¨
    pressure_evaluation: EvaluationInfo  # ì••ë ¥ íŒë‹¨
    final_judgement: str  # "ì •ìƒ", "ì´ìƒ", "ì£¼ì˜"


# LLM API í˜¸ì¶œ ë¶€ë¶„ (ë™ê¸° ë©”ì„œë“œ)
def sync_call(client, usage_burner, data_chunk):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": ANALYZE_SYSTEM_PROMPT.format(
                    usage_burner=json.dumps(usage_burner, ensure_ascii=False, indent=2),
                    data=json.dumps(data_chunk, ensure_ascii=False, indent=2),
                ),
            },
        ],
        max_tokens=4096,
        temperature=0,
    )
    return response.choices[0].message.content


async def async_llm(llm, data_chunk, lock, output_file, max_retries=5):
    client = llm

    with open("./usage_burner.json", "r", encoding="utf-8") as f:
        usage_burner = json.load(f)

    for attempt in range(1, max_retries + 2):  # ì²« ì‹œë„ + max_retriesë²ˆ ì‹œë„
        try:
            # âœ… ë™ê¸° ë©”ì„œë“œë¥¼ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰!
            ai_response = await asyncio.to_thread(
                sync_call, client, usage_burner, data_chunk
            )

            cleaned_response = re.sub(r"```json|```", "", ai_response).strip()
            parsed_json_list = json.loads(cleaned_response)

            structured_data: List[EvaluationOutput] = [
                EvaluationOutput(**entry) for entry in parsed_json_list
            ]

            async with lock:
                with open(output_file, "a", encoding="utf-8") as f:
                    for entry in structured_data:
                        line = json.dumps(entry.model_dump(), ensure_ascii=False)
                        f.write(line + "\n")

            return structured_data  # ì„±ê³µí•˜ë©´ ë¦¬í„´í•˜ê³  ë!

        except Exception as e:
            print(f"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt}/{max_retries + 1}): {e}")
            if attempt == max_retries + 1:
                print("âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼! ì´ chunkëŠ” ë„˜ì–´ê°.")
                return None
            else:
                print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                await asyncio.sleep(2)  # 2ì´ˆ ì‰¬ê³  ì¬ì‹œë„ (í•„ìš”í•˜ë©´ ì¡°ì ˆ ê°€ëŠ¥)


def get_data_from_txt(file_path):
    data_list = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line_data = json.loads(line.strip())
            data_list.append(line_data)
    return data_list


async def batch_process(
    llm,
    all_data_chunks: List[List[Dict]],
    output_file="final_results.jsonl",
    max_concurrency=20,
):
    lock = asyncio.Lock()
    with open(output_file, "w", encoding="utf-8") as f:
        pass

    semaphore = asyncio.Semaphore(max_concurrency)

    async def run_with_semaphore(chunk):
        async with semaphore:
            return await async_llm(llm, chunk, lock, output_file)

    tasks = [
        asyncio.create_task(run_with_semaphore(chunk)) for chunk in all_data_chunks
    ]

    # âœ… gatherë¡œ ë³‘ë ¬ë¡œ í•œë°©ì— ì²˜ë¦¬ (ì§„ì§œ ë³‘ë ¬ ëŠë‚Œ!)
    results = await tqdm_asyncio.gather(*tasks, desc="ì§„í–‰ìƒí™© (í•œë°©ì—!)")

    return results


def count_jsonl_lines_simple(file_path):
    """
    ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•: ë¼ì¸ ìˆ˜ ì„¸ê¸°
    ê° ë¼ì¸ì´ í•˜ë‚˜ì˜ JSON ê°ì²´ë¼ê³  ê°€ì •
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            count = sum(1 for line in f if line.strip())  # ë¹ˆ ë¼ì¸ ì œì™¸
        return count
    except FileNotFoundError:
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return 0
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 0


# if __name__ == "__main__":
#     llm = initialize_llm("gpt4o")
#     result_file = "./final_results.jsonl"
#     # âœ… data_chunks.txtì—ì„œ ì²­í¬ ë¶ˆëŸ¬ì˜¤ê¸°
#     chunk_file = "./test_chunk.txt"
#     all_data_chunks = get_data_from_txt(chunk_file)

#     # âœ… ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰ (Deprecation ê²½ê³  ì—†ì´!)
#     asyncio.run(batch_process(llm, all_data_chunks, output_file=result_file))

#     print("\nâœ… ëª¨ë“  ê²°ê³¼ê°€ 'final_results.jsonl'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

#     count_line = count_jsonl_lines_simple(result_file)
#     print(f"ì´ìƒì¹˜ë¼ê³  íŒë‹¨í•œ ë°ì´í„°ì˜ ì´ ê°œìˆ˜: {count_line}")
