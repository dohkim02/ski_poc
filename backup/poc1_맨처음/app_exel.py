import streamlit as st
import os
import asyncio
from _analyze import *
from _qna import *
from app_utils import *
import concurrent.futures
import threading
import time
from _preprocess import *
from _preprocess_exel import *
import tempfile

# âœ… Streamlit í˜ì´ì§€ ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="[ê³¼ì œ1] ì´ìƒì¹˜ ë°ì´í„° ë¶„ì„ & ì§ˆì˜ ì‘ë‹µ")
st.title("[ê³¼ì œ1] ì´ìƒì¹˜ ë°ì´í„° ë¶„ì„ ë° ì§ˆì˜ ì‘ë‹µ")

# âœ… í˜ì´ì§€ ë„ˆë¹„ ì¡°ì • (ì„ íƒì‚¬í•­)
st.markdown(
    """
<style>
.main .block-container {
    max-width: 800px;
    padding-top: 2rem;
    padding-bottom: 2rem;
}
</style>
""",
    unsafe_allow_html=True,
)

# âœ… ëª¨ë¸ ì´ˆê¸°í™” (1íšŒë§Œ)
if "llm" not in st.session_state:
    st.session_state["llm"] = initialize_llm("gpt4o")

llm = st.session_state["llm"]

# âœ… íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
uploaded_file = st.file_uploader("Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx", "xls"])

# âœ… íŒŒì¼ì´ ì—…ë¡œë“œë˜ê³  ì•„ì§ ë¶„ì„í•˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ë¶„ì„ ì‹¤í–‰
if uploaded_file is not None and "analysis_done" not in st.session_state:
    # ì„ì‹œ íŒŒì¼ë¡œ ì—…ë¡œë“œëœ íŒŒì¼ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp_file:
        tmp_file.write(uploaded_file.read())
        uploaded_file_path = tmp_file.name

    st.success("íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ! ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # âœ… Excel ì „ì²˜ë¦¬ ë‹¨ê³„
    with st.spinner("Excel íŒŒì¼ ì „ì²˜ë¦¬ ì¤‘..."):
        try:
            # 1ï¸âƒ£ ì²« ë²ˆì§¸ í•¨ìˆ˜: ê²°ì¸¡ ì œê±° ë° ì •ë ¬ (sheet2 ê¸°ì¤€)
            st.text("1ë‹¨ê³„: ê²°ì¸¡ê°’ ì œê±° ë° ì •ë ¬ ì¤‘...")
            output_path = drop_na(uploaded_file_path)

            # 2ï¸âƒ£ ë‘ ë²ˆì§¸ í•¨ìˆ˜: ë‹¤ë¥¸ íŒŒì¼ë„ 'êµ¬ë¶„' ì¼ì¹˜ì‹œì¼œ ìƒˆë¡œ ì €ì¥
            st.text("2ë‹¨ê³„: ë°ì´í„° ë¶„í•  ì²˜ë¦¬ ì¤‘...")
            new_output_path = data_split(uploaded_file_path, output_path)

            # 3ï¸âƒ£ ì„¸ ë²ˆì§¸ í•¨ìˆ˜: ë‘ íŒŒì¼ ë³‘í•©
            if os.path.exists(output_path) and os.path.exists(new_output_path):
                st.text("3ë‹¨ê³„: íŒŒì¼ ë³‘í•© ì¤‘...")
                merged_df = data_merge(output_path, new_output_path)
                # 500ì¤„ê¹Œì§€ë§Œ ë¦¬í„´
                merged_df = merged_df.head(500)
                # ê²°ê³¼ë¥¼ ì „ì²˜ë¦¬ëœ íŒŒì¼ë¡œ ì €ì¥
                merged_output_path = "../data/preprocessed_data.xlsx"
                merged_df.to_excel(merged_output_path, index=False)
                st.success("ì „ì²˜ë¦¬ ì™„ë£Œ! ë³‘í•©ëœ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

                # 4ï¸âƒ£ Excelì„ txtë¡œ ë³€í™˜
                st.text("4ë‹¨ê³„: Excelì„ txt ì²­í¬ë¡œ ë³€í™˜ ì¤‘...")
                excel_to_txt_chunks_parallel(
                    merged_output_path, "./uploaded_data.txt", chunk_size=10
                )
                st.success("txt ë³€í™˜ ì™„ë£Œ!")

            else:
                st.error("âš ï¸ ë³‘í•©í•  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í™•ì¸í•´ì£¼ì„¸ìš”!")
                st.stop()

        except Exception as e:
            st.error(f"ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.stop()

    # âœ… ì‹¤ì œ ì§„í–‰ë¥ ì„ ì¶”ì í•˜ì—¬ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    def analyze_with_real_progress():
        all_data_chunks = get_data_from_txt("uploaded_data.txt")
        result_file = "./final_results.jsonl"

        # âœ… ì§„í–‰ë¥  ì¶”ì ê¸° ìƒì„±
        progress_tracker = ProgressTracker()
        analysis_result = {"completed": False, "count": 0, "error": None}

        def run_analysis():
            try:
                asyncio.run(
                    batch_process_with_tracker(
                        llm, all_data_chunks, result_file, 20, progress_tracker
                    )
                )
                count_line = count_jsonl_lines_simple(result_file)
                analysis_result["count"] = count_line
                analysis_result["completed"] = True
            except Exception as e:
                analysis_result["error"] = str(e)
                analysis_result["completed"] = True

        # âœ… ë¶„ì„ì„ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        analysis_thread = threading.Thread(target=run_analysis)
        analysis_thread.start()

        return analysis_thread, progress_tracker, analysis_result

    # âœ… ì§„í–‰ë¥  í‘œì‹œ UI ìƒì„±
    progress_bar = st.progress(0)
    status_text = st.empty()

    with st.spinner("ë¶„ì„ ì¤‘..."):
        analysis_thread, progress_tracker, analysis_result = (
            analyze_with_real_progress()
        )

        # âœ… ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§
        while analysis_thread.is_alive():
            progress_data = progress_tracker.get_progress()
            if progress_data:
                completed, total = progress_data
                progress = completed / total if total > 0 else 0
                progress_bar.progress(progress)
                status_text.text(
                    f"ë¶„ì„ ì§„í–‰ ì¤‘... {completed}/{total} ({progress*100:.1f}%)"
                )

            time.sleep(0.1)  # 100msë§ˆë‹¤ ì²´í¬

        # âœ… ì™„ë£Œ ì²˜ë¦¬
        analysis_thread.join()

        if analysis_result["error"]:
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {analysis_result['error']}")
        else:
            progress_bar.progress(1.0)
            count_line = analysis_result["count"]
            status_text.text(
                f"ë¶„ì„ ì™„ë£Œ! ì´ {count_line}ê°œì˜ ì´ìƒì¹˜ ë°ì´í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            )

            st.success("ëª¨ë“  ê²°ê³¼ê°€ 'final_results.jsonl'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.write(f"ì´ìƒì¹˜ë¼ê³  íŒë‹¨í•œ ë°ì´í„°ì˜ ì´ ê°œìˆ˜: {count_line}")

            # âœ… ë¶„ì„ ì™„ë£Œ ìƒíƒœ ì €ì¥
            st.session_state["analysis_done"] = True
            st.session_state["uploaded_filename"] = uploaded_file.name
            st.session_state["total_count"] = count_line

# âœ… ë¶„ì„ ì™„ë£Œ ìƒíƒœ í‘œì‹œ
if st.session_state.get("analysis_done", False):
    st.info(
        f"ğŸ“Š ë¶„ì„ ì™„ë£Œëœ íŒŒì¼: {st.session_state.get('uploaded_filename', 'unknown')}"
    )
    st.info(f"ğŸ”¢ ì´ìƒì¹˜ ë°ì´í„° ì´ ê°œìˆ˜: {st.session_state.get('total_count', 0)}")


# âœ… ì‚¬ìš©ì ì§ˆì˜ ì²˜ë¦¬ (ë¶„ì„ì´ ì™„ë£Œëœ ê²½ìš°ì—ë§Œ)
if st.session_state.get("analysis_done", False):
    st.markdown("---")
    st.subheader("ğŸ’¬ ì§ˆì˜ì‘ë‹µ")

    query = st.text_input("ğŸ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

    # âœ… ìƒˆë¡œìš´ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if query and (query != st.session_state.get("last_query", "")):
        st.write("ì§ˆë¬¸ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")

        # âœ… ë¹„ë™ê¸° ì§ˆì˜ ì²˜ë¦¬ë„ ThreadPoolExecutorë¡œ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
        def process_query():
            result_file = "./final_results.jsonl"
            chunks = load_jsonl_chunks(result_file, chunk_size=200)
            results = asyncio.run(batch_llm_answers(llm, query, chunks))
            return [r for r in results if r is not None]

        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(process_query)
                all_results = future.result()

        # âœ… ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state["query_results"] = all_results
        st.session_state["last_query"] = query
        st.session_state["page"] = 0  # ìƒˆ ì§ˆë¬¸ ì‹œ ì²« í˜ì´ì§€ë¡œ ë¦¬ì…‹

    # âœ… ì €ì¥ëœ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ í‘œì‹œ
    if "query_results" in st.session_state and st.session_state["query_results"]:
        all_results = st.session_state["query_results"]

        # âœ… 10ê°œì”© í˜ì´ì§€ë¡œ ì¶œë ¥
        chunk_size = 10
        num_chunks = (len(all_results) + chunk_size - 1) // chunk_size

        if "page" not in st.session_state:
            st.session_state["page"] = 0

        page = st.session_state["page"]
        start = page * chunk_size
        end = start + chunk_size

        st.markdown(f"**ì§ˆë¬¸: {st.session_state.get('last_query', '')}**")
        st.markdown(
            f"**ì´ {len(all_results)}ê°œì˜ ê²°ê³¼ ì¤‘ {start+1}-{min(end, len(all_results))}ë²ˆì§¸ ê²°ê³¼:**"
        )

        for i, answer in enumerate(all_results[start:end], start + 1):
            with st.expander(f"ê²°ê³¼ {i}"):
                st.markdown(answer)

        # âœ… í˜ì´ì§€ë„¤ì´ì…˜ ë²„íŠ¼ (rerun ì—†ì´ ìƒíƒœë§Œ ë³€ê²½)
        col1, col2, col3 = st.columns([1, 2, 1])

        with col1:
            if page > 0:
                if st.button("â¬…ï¸ ì´ì „"):
                    st.session_state["page"] -= 1

        with col2:
            st.write(f"í˜ì´ì§€ {page + 1} / {num_chunks}")

        with col3:
            if end < len(all_results):
                if st.button("ë‹¤ìŒ â¡ï¸"):
                    st.session_state["page"] += 1

else:
    st.write("ğŸ‘† ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
