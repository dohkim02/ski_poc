from prompt import QNA_SYSTEM_PROMPT
import pandas as pd
import json
from pydantic import BaseModel
from typing import Dict, List
import sys
import os
import re
import asyncio
from tqdm.asyncio import tqdm_asyncio

# model.pyê°€ ìˆëŠ” í´ë” ê²½ë¡œ
MODEL_PATH = os.path.abspath("../")  # ì˜ˆ: í•œ ë‹¨ê³„ ë°”ê¹¥ í´ë”
sys.path.append(MODEL_PATH)
from model import initialize_llm


# LLM API í˜¸ì¶œ ë¶€ë¶„ (ë™ê¸° ë©”ì„œë“œ)
def sync_call_llm(client, query, data_chunk):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": QNA_SYSTEM_PROMPT.format(
                    data=data_chunk,
                ),
            },
            {"role": "user", "content": query},
        ],
        max_tokens=4096,
        temperature=0,
    )
    return response.choices[0].message.content


async def async_llm_answer(llm, query, data_chunk, max_retries=5):
    client = llm

    for attempt in range(1, max_retries + 2):  # ì²« ì‹œë„ + max_retriesë²ˆ ì‹œë„
        try:
            # âœ… ë™ê¸° ë©”ì„œë“œë¥¼ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰!
            ai_response = await asyncio.to_thread(
                sync_call_llm, client, query, data_chunk
            )

            return ai_response  # ì„±ê³µí•˜ë©´ ë¦¬í„´í•˜ê³  ë!

        except Exception as e:
            print(f"ğŸš¨ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt}/{max_retries + 1}): {e}")
            if attempt == max_retries + 1:
                print("âš ï¸ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼! ì´ chunkëŠ” ë„˜ì–´ê°.")
                return None
            else:
                print("ğŸ”„ ì¬ì‹œë„ ì¤‘...")
                await asyncio.sleep(2)  # 2ì´ˆ ì‰¬ê³  ì¬ì‹œë„ (í•„ìš”í•˜ë©´ ì¡°ì ˆ ê°€ëŠ¥)


async def batch_llm_answers(
    llm,
    query,
    all_data_chunks: List[List[Dict]],
    max_concurrency=20,
):

    semaphore = asyncio.Semaphore(max_concurrency)

    async def run_with_semaphore(chunk):
        async with semaphore:
            return await async_llm_answer(llm, query, chunk)

    tasks = [
        asyncio.create_task(run_with_semaphore(chunk)) for chunk in all_data_chunks
    ]

    # âœ… gatherë¡œ ë³‘ë ¬ë¡œ í•œë°©ì— ì²˜ë¦¬ (ì§„ì§œ ë³‘ë ¬ ëŠë‚Œ!)
    results = await tqdm_asyncio.gather(*tasks, desc="ì§„í–‰ìƒí™©")

    return results


def load_jsonl_chunks(file_path, chunk_size=20):
    chunks = []
    current_chunk = []

    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line_data = json.loads(line.strip())
            current_chunk.append(line_data)

            if len(current_chunk) == chunk_size:
                chunks.append(current_chunk)
                current_chunk = []

    # ë§ˆì§€ë§‰ ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬
    if current_chunk:
        chunks.append(current_chunk)

    return chunks


# if __name__ == "__main__":
#     llm = initialize_llm("gpt4o")

#     result_file = "./final_results.jsonl"
#     chunks = load_jsonl_chunks(result_file)

#     while True:
#         query = input("ì§ˆë¬¸: ")

#         # âœ… ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬
#         all_results = asyncio.run(batch_llm_answers(llm, query, chunks))

#         # âœ… Noneì´ ì•„ë‹Œ ê²°ê³¼ë§Œ ì •ë¦¬
#         all_results = [r for r in all_results if r is not None]

#         # âœ… 10ê°œì”© ë‚˜ëˆ ì„œ ì¶œë ¥
#         chunk_size = 10
#         for i in range(0, len(all_results), chunk_size):
#             batch = all_results[i : i + chunk_size]
#             print("\n".join(batch))
#             input("\n--- ë” ë³´ë ¤ë©´ ì—”í„°ë¥¼ ëˆ„ë¥´ì„¸ìš” ---")
