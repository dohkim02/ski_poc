# File version: 2.1 - Fixed group_index.json path resolution for Streamlit Cloud
from pydantic import BaseModel, Field, model_validator
from typing import Literal, Optional
from langchain_core.prompts import PromptTemplate
import json
from prompt import (
    CLASSIFY_DATA_GROUP,
    PATTERN_CHECK_PROMPT,
)
from utils import (
    get_json,
    get_latest_6month,
    get_previous_monthes,
    get_data_from_txt,
    get_group_usage_info,
    write_outlier,
    write_post_process,
)
import os
import sys
import asyncio
from tqdm import tqdm
import pandas as pd
import ast
import tempfile

# ëª¨ë¸ ê²½ë¡œ ì„¤ì • - Streamlit Cloud í˜¸í™˜
try:
    from model import initialize_llm
except ImportError:
    # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from model import initialize_llm


class Analyze:
    def __init__(
        self,
        llm,
        ground_truth_path=None,
    ):
        self.llm = llm
        self.ground_truth = None  # í•­ìƒ ì†ì„± ìƒì„±

        # Streamlit Cloud í˜¸í™˜ ë°©ì‹ìœ¼ë¡œ ground_truth íŒŒì¼ ì°¾ê¸°
        if ground_truth_path is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))

            # ê°€ëŠ¥í•œ íŒŒì¼ ìœ„ì¹˜ë“¤ (Streamlit Cloudì—ì„œ ë” ì•ˆì „í•œ ìˆœì„œ)
            possible_locations = [
                os.path.join(current_dir, "group_index.json"),  # ê°™ì€ ë””ë ‰í† ë¦¬
                os.path.join(
                    current_dir, "make_instruction", "group_index.json"
                ),  # í•˜ìœ„ ë””ë ‰í† ë¦¬
                "group_index.json",  # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬
                "make_instruction/group_index.json",  # ìƒëŒ€ ê²½ë¡œ
                os.path.join(
                    os.getcwd(), "group_index.json"
                ),  # í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì ˆëŒ€ê²½ë¡œ
                os.path.join(os.getcwd(), "poc2", "group_index.json"),  # poc2 ë””ë ‰í† ë¦¬
            ]

            print(f"ğŸ” Looking for group_index.json in the following locations:")
            for i, path in enumerate(possible_locations, 1):
                exists = os.path.exists(path)
                print(f"  {i}. {path} -> {'âœ“ FOUND' if exists else 'âœ— NOT FOUND'}")
                if exists:
                    ground_truth_path = path
                    print(f"âœ… Using ground_truth file: {ground_truth_path}")
                    break

            if ground_truth_path is None:
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ê¸°ë³¸ êµ¬ì¡°ë¡œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
                print(
                    "âš ï¸  WARNING: group_index.json not found. Creating empty ground truth."
                )
                self.ground_truth = self._create_default_ground_truth()
                return

        try:
            print(f"ğŸ“„ Loading group_index.json from: {ground_truth_path}")
            self.ground_truth = get_json(ground_truth_path)
            print(
                f"âœ… Ground truth loaded successfully with {len(self.ground_truth)} entries"
            )
        except Exception as e:
            print(f"âŒ Error loading ground truth file: {str(e)}")
            print("ğŸ”„ Falling back to default ground truth")
            self.ground_truth = self._create_default_ground_truth()

    def _create_default_ground_truth(self):
        """ê¸°ë³¸ ground truth êµ¬ì¡° ìƒì„±"""
        return {
            "A": {
                "ê±´ì„¤ì—…": {
                    "ì¼ë°˜ìš©1": {
                        "ì €ì••": {
                            "category": "ê±´ì„¤ì—…",
                            "standard": {
                                "1ì›”": 100,
                                "2ì›”": 100,
                                "3ì›”": 100,
                                "4ì›”": 100,
                                "5ì›”": 100,
                                "6ì›”": 100,
                                "7ì›”": 100,
                                "8ì›”": 100,
                                "9ì›”": 100,
                                "10ì›”": 100,
                                "11ì›”": 100,
                                "12ì›”": 100,
                            },
                            "data_num": 10,
                        }
                    }
                }
            }
        }

    # ì—…íƒœì™€ ì—…ì¢…ì„ ê¸°ë°˜ìœ¼ë¡œ ê·¸ë£¹ ë¶„ë¥˜ í›„, ìš©ë„ íŒŒì•…í•˜ì—¬ ê¸°ì¤€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    async def classify_llm(self, data):
        class Classify(BaseModel):
            result: str = Field(
                description='ë¶„ë¥˜ëœ ê·¸ë£¹ì˜ ìˆ«ì. ë°˜ë“œì‹œ ë¬¸ìì—´ ìˆ«ì í•˜ë‚˜ì—¬ì•¼ í•¨. "0"ë¶€í„° "11" ì‚¬ì´ ì¤‘ í•˜ë‚˜.""'
            )

        structured_llm = self.llm.with_structured_output(Classify)
        input_data = str(data["ì—…íƒœ"]) + "(" + str(data["ì—…ì¢…"]) + ")"
        prompt = PromptTemplate.from_template(CLASSIFY_DATA_GROUP)
        chain = prompt | structured_llm
        result = await asyncio.to_thread(chain.invoke, {"data": input_data})
        category_lst = {
            "0": "ê±´ì„¤ì—…",
            "1": "êµìœ¡ì—…",
            "2": "ê¸ˆìœµë¶€ë™ì‚°ì—…",
            "3": "ê¸°íƒ€",
            "4": "ë„ì†Œë§¤ì—…",
            "5": "ë¹„ì˜ë¦¬ë²•ì¸",
            "6": "ì„œë¹„ìŠ¤ì—…",
            "7": "ìˆ™ë°• ë° ìŒì‹ì ì—…",
            "8": "ì‹œì„¤ê´€ë¦¬ì—…",
            "9": "ìš´ìˆ˜ì—…",
            "10": "ì •ë³´í†µì‹ ì—…",
            "11": "ì œì¡°ì—…",
        }
        group_id = result.result

        category = category_lst[group_id]
        grade = data.get("ë“±ê¸‰", "A")  # ê¸°ë³¸ê°’ A
        usage = data.get("ìš©ë„", "ì¼ë°˜ìš©1")  # ê¸°ë³¸ê°’ ì¼ë°˜ìš©1
        # ì•ˆì „í•œ í‚¤ ì ‘ê·¼ìœ¼ë¡œ ë³€ê²½
        pressure = data.get("ì••ë ¥_ê·¸ë£¹", "ì €ì••")  # ê¸°ë³¸ê°’ì„ "ì €ì••"ìœ¼ë¡œ ì„¤ì •

        ground_truth = get_group_usage_info(
            self.ground_truth, grade, category, usage, pressure
        )
        import pdb

        pdb.set_trace()

        # ground_truthê°€ ë¬¸ìì—´ì¸ ê²½ìš° (ì—ëŸ¬ ë©”ì‹œì§€) ê¸°ë³¸ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
        if isinstance(ground_truth, str):
            print(
                f"Warning: get_group_usage_info returned error message: {ground_truth}"
            )
            # ê¸°ë³¸ êµ¬ì¡°ë¥¼ ê°€ì§„ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
            return {
                "category": f"({grade}, {category}, {usage}, {pressure})",
                "standard": {},
                "data_num": 0,
            }

        return ground_truth

    def find_outlier(
        self,
        standard_data: dict,
        new_data: dict,
        min_consecutive: int = 3,
        percentage=0.7,
    ) -> list:
        """
        ê¸°ì¤€ ë°ì´í„° ëŒ€ë¹„ ì‹ ê·œ ë°ì´í„°ê°€ í•˜í•œ ë¯¸ë§Œì´ê±°ë‚˜ ìƒí•œ ì´ˆê³¼ì¸ ì›”ì„ ì°¾ì•„,
        ì—°ì†ìœ¼ë¡œ min_consecutiveê°œì›” ì´ìƒì¸ êµ¬ê°„ë§Œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
        ë‹¨, new_data ê°’ì´ 0.0ì¸ ì›”ì€ ì œì™¸.

        0.0ì´ 3ê°œì›” ì´ìƒ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.

        Args:
            standard_data (dict): ê¸°ì¤€ ë°ì´í„° (ì˜ˆ: {"1ì›”": 530.5, ...})
            new_data (dict): ì‹ ê·œ ë°ì´í„° (ì˜ˆ: {"1ì›”": 287.96, ...})
            min_consecutive (int): ìµœì†Œ ì—°ì† ê°œì›” ìˆ˜ (ê¸°ë³¸ê°’: 3)
            percentage (float): í•˜í•œ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.7, ìƒí•œì€ 1/percentage)

        Returns:
            List of dicts: ì—°ì† êµ¬ê°„ë³„ ì •ë³´
            (ì˜ˆ: [{'months': ['6ì›”', '7ì›”', '8ì›”'], 'types': ['í•˜í•œ', 'í•˜í•œ', 'ìƒí•œ']}])
        """
        # ì›” ì´ë¦„ì„ ìˆ«ìë¡œ ë§¤í•‘
        month_to_num = {
            "1ì›”": 1,
            "2ì›”": 2,
            "3ì›”": 3,
            "4ì›”": 4,
            "5ì›”": 5,
            "6ì›”": 6,
            "7ì›”": 7,
            "8ì›”": 8,
            "9ì›”": 9,
            "10ì›”": 10,
            "11ì›”": 11,
            "12ì›”": 12,
        }

        # ì‹¤ì œ ë°ì´í„°ì— ìˆëŠ” ì›”ë“¤ë§Œ ì¶”ì¶œí•˜ê³  ìˆœì„œëŒ€ë¡œ ì •ë ¬
        available_months = list(set(standard_data.keys()) & set(new_data.keys()))
        if not available_months:
            return []

        # ì›”ì„ ìˆ«ì ìˆœì„œëŒ€ë¡œ ì •ë ¬
        months_order = sorted(available_months, key=lambda x: month_to_num[x])

        # ë°ì´í„° ê°œìˆ˜ê°€ min_consecutiveë³´ë‹¤ ì‘ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if len(months_order) < min_consecutive:
            return []

        # 0.0ì´ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì²´í¬
        consecutive_zeros = 0
        max_consecutive_zeros = 0

        for month in months_order:
            if new_data[month] == 0.0:
                consecutive_zeros += 1
                max_consecutive_zeros = max(max_consecutive_zeros, consecutive_zeros)
            else:
                consecutive_zeros = 0

        # 0.0ì´ min_consecutive-1ê°œì›” ì´ìƒ ì—°ì†ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if max_consecutive_zeros >= min_consecutive - 1:
            return []

        # ìƒí•œ ë¹„ìœ¨ ê³„ì‚° (í•˜í•œì˜ ì—­ìˆ˜)
        upper_percentage = 1.5

        # ê° ì›”ì˜ ì´ìƒì¹˜ ì—¬ë¶€ì™€ íƒ€ì…(í•˜í•œ/ìƒí•œ) ì²´í¬
        outlier_info = []
        for month in months_order:
            if new_data[month] == 0.0:
                outlier_info.append({"is_outlier": False, "type": None})
            elif new_data[month] < standard_data[month] * percentage:
                outlier_info.append({"is_outlier": True, "type": "í•˜í•œ"})
            elif new_data[month] > standard_data[month] * upper_percentage:
                outlier_info.append({"is_outlier": False, "type": "ìƒí•œ"})
            else:
                outlier_info.append({"is_outlier": False, "type": None})

        # ë§ˆì§€ë§‰ min_consecutiveê°œì›”ì´ ì´ìƒì¹˜ê°€ ì•„ë‹Œì§€ í™•ì¸ (ë°ì´í„° ê¸¸ì´ê°€ ì¶©ë¶„í•œ ê²½ìš°ë§Œ)
        last_months_not_outliers = True
        if len(outlier_info) >= min_consecutive:
            last_months_not_outliers = all(
                not outlier_info[i]["is_outlier"] for i in range(-min_consecutive, 0)
            )

        # ì „ì²´ ì›”ì´ ëª¨ë‘ ì´ìƒì¹˜ì¸ì§€ í™•ì¸
        all_months_are_outliers = all(info["is_outlier"] for info in outlier_info)

        result_sequences = []
        start = None

        for i, info in enumerate(outlier_info):
            if info["is_outlier"]:
                if start is None:
                    start = i
            else:
                if start is not None and i - start >= min_consecutive:
                    months_in_sequence = months_order[start:i]
                    types_in_sequence = [
                        outlier_info[j]["type"] for j in range(start, i)
                    ]
                    result_sequences.append(
                        {"months": months_in_sequence, "types": types_in_sequence}
                    )
                start = None

        # ë§ˆì§€ë§‰ê¹Œì§€ ì—°ì†ëœ ê²½ìš° ì²˜ë¦¬
        if (
            start is not None
            and len(months_order) - start >= min_consecutive
            and not (last_months_not_outliers and len(outlier_info) >= min_consecutive)
            and not all_months_are_outliers
        ):
            months_in_sequence = months_order[start:]
            types_in_sequence = [
                outlier_info[j]["type"] for j in range(start, len(months_order))
            ]
            result_sequences.append(
                {"months": months_in_sequence, "types": types_in_sequence}
            )

        # ì „ì²´ê°€ ì´ìƒì¹˜ì¸ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        if all_months_are_outliers:
            return []

        return result_sequences

    async def judge_with_biz_llm(self, ground_truth, data):
        try:
            # ground_truthê°€ ë¬¸ìì—´ì¸ ê²½ìš° JSONìœ¼ë¡œ íŒŒì‹± ì‹œë„
            if isinstance(ground_truth, str):
                try:
                    ground_truth = json.loads(ground_truth)
                except json.JSONDecodeError:
                    print(
                        f"Warning: Could not parse ground_truth as JSON: {ground_truth}"
                    )
                    return [], {}, {}

            # ground_truthê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ê²½ìš° ì²˜ë¦¬
            if not isinstance(ground_truth, dict):
                print(
                    f"Warning: ground_truth is not a dictionary: {type(ground_truth)}"
                )
                return [], {}, {}

            # í•„ìš”í•œ í‚¤ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
            required_keys = ["category", "standard", "data_num"]
            missing_keys = [key for key in required_keys if key not in ground_truth]
            if missing_keys:
                print(f"Warning: Missing keys in ground_truth: {missing_keys}")
                return [], {}, {}

            # ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (ë‹¨ì¼ í–‰)
            df = pd.DataFrame([data])

            # 'heat_input' ì»¬ëŸ¼ ìƒì„± (ë³´ì¼ëŸ¬ ì—´ëŸ‰ + ì—°ì†Œê¸° ì—´ëŸ‰)
            df["category"] = ground_truth["category"]
            # ê¸°ì¡´ ì»¬ëŸ¼ ì•ˆì „í•˜ê²Œ ì‚­ì œ (ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ)
            columns_to_drop = [
                col for col in ["ì—…íƒœ", "ì—…ì¢…", "ìš©ë„", "ë“±ê¸‰"] if col in df.columns
            ]
            if columns_to_drop:
                df = df.drop(columns_to_drop, axis=1)
            # DataFrameì„ ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            data = df.iloc[0].to_dict()

            # ground_truth = json.dumps(ground_truth, ensure_ascii=False)
            gt = ground_truth["standard"]

            # ì•ˆì „í•œ í‚¤ ì ‘ê·¼ìœ¼ë¡œ ë³€ê²½
            years_data = data.get("3ë…„ì¹˜ ë°ì´í„°", {})
            input_data = get_latest_6month(years_data)
            # gt ë”•ì…”ë„ˆë¦¬ì—ë„ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
            standard_data = {month: gt.get(month, 0) for month in input_data.keys()}

            if ground_truth["data_num"] > 100:
                result = self.find_outlier(standard_data, input_data)
            else:
                result = []

            # ì‹¤ì œ ë¹„êµì— ì‚¬ìš©ëœ ë°ì´í„°ë“¤ë„ í•¨ê»˜ ë°˜í™˜
            return result, standard_data, input_data

        except Exception as e:
            print(f"Error in judge_with_biz_llm: {str(e)}")
            print(f"ground_truth type: {type(ground_truth)}")
            print(f"ground_truth value: {ground_truth}")
            return [], {}, {}

    async def process_single_item(self, data_item):
        ground_truth = await self.classify_llm(data_item)
        judge_result, standard_data, input_data = await self.judge_with_biz_llm(
            ground_truth, data_item
        )

        return {
            "judge_result": judge_result,
            "input_data": data_item,
            "ground_truth": ground_truth,
            "standard_data": standard_data,  # ì‹¤ì œ ë¹„êµì— ì‚¬ìš©ëœ ê¸°ì¤€ ë°ì´í„°
            "comparison_input_data": input_data,  # ì‹¤ì œ ë¹„êµì— ì‚¬ìš©ëœ ì…ë ¥ ë°ì´í„°
        }

    async def pattern_checker(self, years_data):
        class Pattern(BaseModel):
            result: Literal["yes", "no"] = Field(
                description='ì´ì „ë…„ë„ë“¤ì˜ ë°ì´í„° íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ ìµœê·¼ ë°ì´í„° íŒ¨í„´ì˜ ì´ìƒì¹˜ ìœ ë¬´ í™•ì¸. "yes" ë˜ëŠ” "no"ë¡œ ë‹µë³€'
            )
            reason: Optional[str] = Field(
                default=None,
                description='"result"ê°€ "yes"ì¸ ê²½ìš°ì—ë§Œ ì´ìƒí•˜ë‹¤ê³  íŒë‹¨í•œ ì´ìœ ë¥¼ ìì„¸íˆ ì„¤ëª…. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´.',
            )

        keys = list(years_data.keys())
        values = list(years_data.values())

        # ì•ˆì „í•œ ë°ì´í„° ë³€í™˜ í•¨ìˆ˜
        def safe_eval(value):
            if isinstance(value, (dict, list)):
                return value
            elif isinstance(value, str):
                try:
                    return ast.literal_eval(value)
                except (ValueError, SyntaxError):
                    return value
            else:
                return value

        structured_llm = self.llm.with_structured_output(Pattern)
        prompt = PromptTemplate.from_template(PATTERN_CHECK_PROMPT)
        chain = prompt | structured_llm
        result = await asyncio.to_thread(
            chain.invoke,
            {
                "key0": keys[0],
                "key1": keys[1],
                "key2": keys[2],
                "key3": keys[3],
                "value0": safe_eval(values[0]),
                "value1": safe_eval(values[1]),
                "value2": safe_eval(values[2]),
                "value3": safe_eval(values[3]),
            },
        )

        return result

    async def run_biz_judge(self, data_lst):
        """ë¹„ë™ê¸°ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ë˜ ìˆœì„œë¥¼ ë³´ì¥"""

        # í”„ë¡œê·¸ë ˆìŠ¤ë°” ì´ˆê¸°í™”
        progress_bar = tqdm(total=len(data_lst), desc="Processing data", unit="item")

        results = []

        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ (ì„ íƒì‚¬í•­)
        semaphore = asyncio.Semaphore(50)  # ìµœëŒ€ 50ê°œ ë™ì‹œ ì‹¤í–‰

        async def process_with_progress(data_item):
            async with semaphore:
                result = await self.process_single_item(data_item)
                progress_bar.update(1)
                return result

        # ëª¨ë“  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ë˜ ìˆœì„œ ë³´ì¥
        tasks = [process_with_progress(data_item) for data_item in data_lst]
        results = await asyncio.gather(*tasks)

        progress_bar.close()
        return results

    async def run_pattern_check(self, outlier_results):
        """outlier_resultsì— ëŒ€í•´ pattern_checkerë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰"""

        # í”„ë¡œê·¸ë ˆìŠ¤ë°” ì´ˆê¸°í™”
        progress_bar = tqdm(
            total=len(outlier_results), desc="Pattern checking", unit="item"
        )

        results = []

        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ
        semaphore = asyncio.Semaphore(50)  # ìµœëŒ€ 50ê°œ ë™ì‹œ ì‹¤í–‰

        async def process_pattern_check(outlier_item):
            async with semaphore:
                # standard_6_month = outlier_item["standard_data"]

                # latest_6_month_data = outlier_item["comparison_input_data"]
                standard = outlier_item["ground_truth"]["standard"]
                # ì•ˆì „í•œ í‚¤ ì ‘ê·¼ìœ¼ë¡œ ë³€ê²½
                years_data = outlier_item["input_data"].get("3ë…„ì¹˜ ë°ì´í„°", {})

                # ì•ˆì „í•œ ë°ì´í„° ë³€í™˜
                if isinstance(years_data, str):
                    try:
                        years_data = ast.literal_eval(years_data)
                    except (ValueError, SyntaxError):
                        print(f"Warning: Could not parse years_data: {years_data}")
                        progress_bar.update(1)
                        return outlier_item
                elif not isinstance(years_data, dict):
                    print(
                        f"Warning: years_data is not a valid format: {type(years_data)}"
                    )
                    progress_bar.update(1)
                    return outlier_item

                rest_month_data = get_previous_monthes(years_data)
                pattern_result = await self.pattern_checker(years_data)
                progress_bar.update(1)

                # ê¸°ì¡´ outlier_itemì— pattern_result ì¶”ê°€
                result_item = outlier_item.copy()
                result_item["pattern_result"] = pattern_result
                return result_item

        # ëª¨ë“  ì‘ì—…ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ë˜ ìˆœì„œ ë³´ì¥
        tasks = [
            process_pattern_check(outlier_item) for outlier_item in outlier_results
        ]
        results = await asyncio.gather(*tasks)

        progress_bar.close()
        return results


# ì‚¬ìš© ì˜ˆì‹œ:
async def main():
    llm = initialize_llm("langchain_gpt4o")

    # Streamlit Cloud í˜¸í™˜: ì„ì‹œ íŒŒì¼ë¡œ ë°ì´í„° ì²˜ë¦¬
    try:
        # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ preprocessed.txt ì°¾ê¸°
        current_dir = os.path.dirname(__file__)
        possible_data_paths = [
            os.path.join(current_dir, "preprocessed.txt"),
            "preprocessed.txt",
            os.path.join(os.getcwd(), "preprocessed.txt"),
            os.path.join(os.getcwd(), "poc2", "preprocessed.txt"),
        ]

        data_file_path = None
        for path in possible_data_paths:
            if os.path.exists(path):
                data_file_path = path
                break

        if data_file_path is None:
            print("âŒ preprocessed.txt file not found in any expected location")
            return []

        print(f"ğŸ“„ Loading data from: {data_file_path}")
        data_lst = get_data_from_txt(data_file_path)
        print(f"âœ… Loaded {len(data_lst)} items")
    except Exception as e:
        print(f"âŒ Error loading data: {str(e)}")
        return []

    print(f"ğŸ”„ Total items to process: {len(data_lst)}")

    analyzer = Analyze(llm)
    results = await analyzer.run_biz_judge(data_lst)

    # 'ì´ìƒ'ì¸ ê²°ê³¼ë§Œ í•„í„°ë§ (judge_resultê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°)
    outlier_results = [
        item for item in results if item["judge_result"]  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê²½ìš°
    ]

    # outlier_resultsì— ëŒ€í•´ pattern_checker ë³‘ë ¬ ì‹¤í–‰
    if outlier_results:
        print(f"ğŸ” Running pattern check on {len(outlier_results)} outlier cases...")
        outlier_results = await analyzer.run_pattern_check(outlier_results)

    # Streamlit Cloud í˜¸í™˜: ì„ì‹œ íŒŒì¼ë¡œ ê²°ê³¼ ì €ì¥
    try:
        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".txt", delete=False, encoding="utf-8"
        ) as tmp_file:
            output_path = tmp_file.name
            write_outlier(output_path, outlier_results)
            print(f"ğŸ’¾ Outlier results saved to temp file: {output_path}")

        # pattern_checker ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ë¶„ì„ ê²°ê³¼ ì €ì¥
        write_post_process(outlier_results)
    except Exception as e:
        print(f"âš ï¸  Warning: Could not save results to file: {str(e)}")

    return results


# # ì‹¤í–‰
if __name__ == "__main__":
    results = asyncio.run(main())
#     # import pdb

# pdb.set_trace()
